\
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}

\title{Neural Machine Translation with PyTorch: RNN+Attention vs Transformer}

\author{\IEEEauthorblockN{Your Name}
\IEEEauthorblockA{\textit{Department of Informatics} \\
\textit{Your University}\\
City, Country \\
youremail@domain.com}
}

\begin{document}

\maketitle

\begin{abstract}
This paper presents a bilingual neural machine translation system built in PyTorch. We implement a baseline encoder--decoder with attention and compare it to a Transformer. We evaluate on the ManyThings/Tatoeba dataset using SacreBLEU and chrF and include ablation studies on vocabulary size and decoding strategies.
\end{abstract}

\begin{IEEEkeywords}
Neural Machine Translation, Transformer, Attention, PyTorch, SacreBLEU
\end{IEEEkeywords}

\section{Introduction}
Briefly motivate machine translation and deep learning approaches. Summarize contributions: (i) clean data pipeline; (ii) RNN+Attn baseline; (iii) Transformer with warmup and label smoothing; (iv) evaluation and ablations.

\section{Related Work}
Cover SMT $\rightarrow$ NMT shift, attention \cite{bahdanau2014neural}, sequence-to-sequence \cite{sutskever2014sequence}, Transformer \cite{vaswani2017attention}, tokenization methods (BPE/SentencePiece).

\section{Method}
\subsection{Data Preprocessing}
Describe cleaning, train/valid/test splits, SentencePiece BPE and vocabulary sizes.

\subsection{Baseline: RNN + Attention}
Architecture, hidden sizes, dropout, training objective.

\subsection{Transformer}
Encoder--decoder, positional encoding, label smoothing, learning rate warmup.

\section{Experiments}
\subsection{Setup}
Dataset stats; hyperparameters; hardware; training time. Include training/validation loss curves (Fig.~\ref{fig:loss}).

\subsection{Results}
Report SacreBLEU (and chrF) on test set. Include example translations and error analysis.

\subsection{Ablation Study}
At least one variable (e.g., vocab size 4k vs 8k vs 16k, or beam size 1 vs 4).

\section{Conclusion}
Summarize findings and future work.

\section*{Acknowledgment}
(optional) Acknowledge funding or collaborators.

\bibliographystyle{IEEEtran}
\bibliography{refs}

\end{document}
